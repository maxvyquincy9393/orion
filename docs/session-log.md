# Orion — Session Log & Conversation Documentation

> This file documents all major decisions, directions, and progress made across all sessions.
> Update this file at the end of every session before pushing.
> If starting a new session with a new AI assistant, read SKILL.md + this file first.

---

## Session 1 — Project Genesis
**Date:** February 2026  
**Tools used:** Claude (claude.ai)

### What was decided

**Vision origin:** Inspired by AI from science fiction — JARVIS (Iron Man), TARS (Interstellar). The goal is not to build another chatbot but a persistent AI companion that behaves like a real intelligent partner.

**Core differentiation identified:**
- Most AI tools are reactive — wait to be asked
- Orion is proactive — reaches out like a friend via WhatsApp-style persistent threads
- Not one message then done — stateful conversation threads with follow-up capability
- Runs as a background daemon, always on, not just when app is open

**Project named:** Orion  
**Repo created:** github.com/maxvyquincy9393/orion

---

## Session 2 — Architecture Decisions

### What was decided

**Backend language:** Python — chosen because all top AI libraries (LangChain, LangGraph, Whisper, vector DB clients) are Python-native. Fastest path to build.

**Multi-engine LLM strategy:** Not locked to one model. Smart routing per task type:
- Claude → complex reasoning, long context
- GPT-4o → code, structured tasks
- Gemini → multimodal, vision, voice
- Ollama → local, free, privacy-sensitive tasks

**Auth approach:**
- OpenAI → OAuth2 Authorization Code Flow
- Google Gemini → OAuth2
- Anthropic → API Key (OAuth not yet supported)
- Ollama → no auth, localhost

**Agent framework:** LangGraph + LangChain  
- LangChain for RAG pipeline and chains  
- LangGraph for multi-step autonomous agent tasks  
- Enables complex task graphs that don't collapse mid-execution

**Vision layer added:** Live camera + screen capture, like Gemini Live  
- OpenCV for capture  
- Frame sampling with motion detection (not every frame)  
- Gemini Vision or GPT-4V for processing  
- 4 modes: passive / active / on-demand / screen

**Permission sandbox added:** Every system action must pass through sandbox.check() first  
- User edits permissions.yaml to control what Orion can and cannot do  
- Granular per action: FILE_READ, FILE_WRITE, FILE_DELETE, TERMINAL_RUN, APP_OPEN, etc.  
- require_confirm: true = Orion asks via Telegram before acting  
- Hot reload — change permissions without restarting

**Autonomous browsing — 100% free:**
- Playwright + browser-use for navigation
- DuckDuckGo (no API key needed)
- SearXNG self-hosted as search backend

**System control capabilities:**
- File read/write/delete
- Terminal commands (with confirm)
- App open/close
- Calendar read/write
- Mouse/keyboard (disabled by default)

**Cost strategy:** Maximize free tiers and open source throughout. No mandatory paid services.
- Whisper → local, free
- Coqui TTS → local, free
- Chroma → local vector DB, free fallback
- Supabase → free tier primary vector DB

---

## Session 3 — AI Coding Stack Setup

### What was decided

**AI coding assistant stack:**
- GitHub Copilot → inline suggestions in VSCode
- OpenCode → Claude-powered, complex reasoning and refactor
- Gemini CLI → backup when Copilot hits limit
- Codex → scaffold and generate large file structures

**Switching strategy:**
- Copilot limit hit → switch to OpenCode (Claude Haiku 4.5)
- OpenCode limit → Gemini CLI
- Codex for scaffold only, not complex logic

**Auto-push workflow:** `scripts/autopush.py` using watchdog  
- Every file save triggers: git add → git commit → git push  
- 3 second debounce to avoid spam commits  
- Commit format: `auto: [timestamp] [filename]`

**MANDATORY RULES established (in SKILL.md):**
1. Always commit — every change, even WIP
2. Always document — every file, function, and decision

---

## Session 4 — Build Progress

### Phase 1 Implementation Log

**Scaffold** — 43 files, 3028 lines generated by Codex  
All interfaces match SKILL.md exactly.

**config.py** ✅
- Loads .env via python-dotenv
- Typed module-level constants grouped into 8 sections
- validate_required_for_engine() per engine on demand
- as_dict() safe debug dump with masked secrets

**permissions/permission_types.py** ✅
- PermissionAction enum with 11 actions
- ACTION_TO_SECTION mapping
- PermissionResult frozen dataclass

**permissions/config_loader.py** ✅
- Thread-safe RLock
- Hot reload via reload()
- Validates all 11 required sections on load

**permissions/sandbox.py** ✅
- check(action, details) → PermissionResult
- Per-action logic: file path filtering, blocked commands, allowed apps, domain blocking
- request_confirm() → sends Telegram message, polls 30s for yes/no
- Logs to logs/permissions.log

**database/models.py** ✅
- 7 ORM models: User, Message, Session, Memory, Thread, CompressedMemory, TriggerLog
- UUID primary keys, timezone-aware DateTime
- MessageRole enum, ThreadState enum
- Composite indexes for performance
- get_engine(), get_session(), create_all_tables(), drop_all_tables()

**database/vector_store.py** ✅
- Dual backend: Supabase pgvector (primary) + Chroma local (fallback)
- Auto-detection based on config
- Embedding via LangChain (OpenAI or Ollama nomic-embed-text)
- embed(), upsert(), search(), delete(), get_store_stats()

**core/memory.py** ✅
- save_message() — dual write PostgreSQL + vector store
- get_history() — PostgreSQL ordered by timestamp
- get_relevant_context() — vector similarity search scoped to user_id
- compress_old_sessions() — LLM summarization, stores CompressedMemory, deletes old messages

**engines/base.py** ✅
- Abstract base class
- format_messages() concrete method — converts to OpenAI message format

**engines/openai_engine.py** ✅ — OAuth2 via OPENAI_ACCESS_TOKEN  
**engines/claude_engine.py** ✅ — API key auth, system prompt handling  
**engines/gemini_engine.py** ✅ — OAuth2 via GOOGLE_ACCESS_TOKEN  
**engines/local_engine.py** ✅ — Ollama REST API, streaming via JSON lines

**core/rag.py** ✅
- ingest() — chunk via RecursiveCharacterTextSplitter (512/50), embed, upsert
- ingest_file() — auto-detect pdf/txt/md/docx, load, chunk, ingest
- query() — embed question, vector search, return top_k
- build_context() — formatted context string with source attribution
- delete_document()

**core/orchestrator.py** ✅
- route(task_type) — priority routing per task
- route_to_agent(task) — keyword-based agent type detection
- get_available_engines() — startup availability check

**core/context.py** done
- build() assembles full context: last 20 messages + RAG context + relevant past context + system prompt
- get_system_prompt() defines Orion personality
- truncate_context() handles context window limits

**agents/state.py** done
- AgentState TypedDict matching SKILL.md exactly

**agents/nodes.py** done
- supervisor_node, memory_node, summarize_node fully implemented
- search_node, code_node as stubs (Phase 2)

**agents/graph.py** done
- OrionAgentGraph with LangGraph StateGraph
- Nodes: start → supervisor → [search, memory, summarize] → end
- run() and stream_run() implemented

**scripts/autopush.py** done
- AutoPusher class with watchdog, 3 second debounce
- Ignores .env, __pycache__, logs/, .git/
- Logs to logs/autopush.log

**main.py** done
- CLI chat loop: input → context.build() → orchestrator.route() → engine.generate() → memory.save_message()
- Handles exit and quit
- Prints engine status on startup

### Phase 1 first run results (python main.py)

| Component | Status | Note |
|---|---|---|
| Database | Failed | PostgreSQL not running on localhost:5432 — expected |
| Claude | Offline | No ANTHROPIC_API_KEY in .env — expected |
| OpenAI | Offline | No OPENAI_ACCESS_TOKEN — expected |
| Gemini | Offline | No GOOGLE_ACCESS_TOKEN — expected |
| Local | Offline | Ollama not running — expected |
| CLI Loop | Working | Prompts for input, exits on quit |
| Permissions | Working | Defaults loaded |

All failures are expected — no credentials set up yet. Architecture is sound.

### Next steps to get first live conversation
1. Add ANTHROPIC_API_KEY to .env for fastest path to working engine
2. OR run `ollama serve` and `ollama pull llama3` for free local inference
3. Run `python main.py` again — first real conversation with Orion

---

## Phase Checklist

### Phase 1 — Foundation
- [x] OAuth2 setup structure (openai_oauth.py, google_oauth.py)
- [x] Project scaffold — 43 files
- [x] Permission engine (sandbox, config_loader, permission_types, permissions.yaml)
- [x] PostgreSQL schema and SQLAlchemy models
- [x] Vector DB setup (Supabase + Chroma fallback)
- [x] RAG pipeline (ingest, query, build_context)
- [x] All 4 LLM engines (GPT, Claude, Gemini, Ollama)
- [x] Orchestrator routing
- [x] Persistent memory (save, retrieve, compress)
- [x] context.py — context window builder
- [x] agents/ — LangGraph scaffold (state, graph, nodes)
- [x] scripts/autopush.py
- [x] main.py — first working CLI chat loop
- [x] docs/ folder complete

Phase 1 COMPLETE.

### Phase 2 — Proactive Engine
- [x] Background daemon process (background/process.py - OrionDaemon class)
- [x] Trigger detection system (background/triggers.py - TriggerEngine, TriggerType enum)
- [x] Thread state manager (background/thread_manager.py - open_thread, update_state, get_pending_threads, should_follow_up)
- [x] Telegram delivery (delivery/messenger.py - send, send_with_confirm, get_latest_reply, set_webhook)
- [x] Autonomous browser agent (browser/agent.py - BrowserAgent with search_and_summarize, research, navigate_and_extract)
- [x] Free search (browser/search.py - DuckDuckGo HTML scrape + SearXNG fallback, no API key needed)
- [x] System control: file, terminal, calendar (system/file_ops.py, system/terminal.py, system/calendar_ops.py)
- [x] Permission confirmation flow (all system/browser actions call sandbox.check() and sandbox.request_confirm())
- [x] LangGraph tools: search, browse, file, calendar (agents/tools.py - 10 tools registered)

Phase 2 COMPLETE.

**Phase 2 Implementation Details:**

**delivery/messenger.py:**
- send() - Telegram Bot API message sending
- send_with_confirm() - Send + poll for yes/no reply
- get_latest_reply() - Poll for new messages
- set_webhook() - Production webhook setup
- Logs to logs/delivery.log

**background/triggers.py:**
- TriggerType enum: TIME_BASED, SCHEDULE, PATTERN, INACTIVITY, KEYWORD
- Trigger dataclass with id, type, condition, message_template, last_fired, enabled
- TriggerEngine class with load_triggers(), evaluate(), get_fired_triggers(), build_message(), mark_fired()
- Default triggers.yaml: morning check-in (8am), inactivity (4 hours), end of day summary (6pm)
- Logs to logs/triggers.log

**background/thread_manager.py:**
- open_thread(user_id, trigger) - Creates Thread record, returns thread_id
- update_state(thread_id, state) - Updates Thread.state (open/waiting/resolved)
- get_pending_threads(user_id) - Returns all non-resolved threads
- should_follow_up(thread_id) - True if waiting > 1 hour
- Logs to logs/threads.log

**background/process.py:**
- OrionDaemon class with start(), stop(), _loop()
- Runs every 60 seconds in separate thread
- Builds context: current time, day, last message time, pending threads
- Calls trigger_engine.get_fired_triggers()
- For each trigger: checks sandbox permission, opens thread, sends message
- Checks pending threads for follow-ups
- Respects quiet_hours (22:00 - 08:00 by default)
- Logs to logs/daemon.log

**browser/search.py:**
- search(query, max_results) - DuckDuckGo HTML scrape, SearXNG fallback
- _duckduckgo_search() - No API key, pure HTML scraping
- _searxng_search() - Self-hosted search backend
- search_images(), search_news() helper functions
- Logs to logs/browser.log

**browser/playwright_client.py:**
- PlaywrightClient class with async context manager support
- navigate(url) - Return page text content
- screenshot(url) - Return PNG bytes
- extract_links(url) - Return all href links
- extract_content(url, selector) - Scoped text extraction
- click(), fill(), wait_for_selector() for interaction
- All actions check BROWSER_NAVIGATE permission
- Logs to logs/browser.log

**browser/agent.py:**
- BrowserAgent class
- search_and_summarize(query) - Search, visit top 3, summarize via LLM
- research(topic, depth=2) - Multi-level research with link following
- navigate_and_extract(url, goal) - Goal-oriented extraction
- All actions check sandbox permission
- Logs to logs/browser.log

**system/file_ops.py:**
- read_file(path) - Returns file contents
- write_file(path, content) - Writes content
- delete_file(path) - Deletes file
- list_dir(path) - Lists directory
- create_dir(path), copy_file(src, dst), file_exists(), get_file_info()
- Every function calls sandbox.check() with FILE_READ/FILE_WRITE/FILE_DELETE

**system/terminal.py:**
- run(command, timeout) - Returns stdout, stderr, exit_code
- run_safe(command) - Always checks blocked_commands list
- run_background(command) - Returns process ID
- BLOCKED_COMMANDS list: rm -rf, sudo, format, etc.
- Calls sandbox.check(TERMINAL_RUN) with request_confirm()

**system/calendar_ops.py:**
- get_events(date) - Returns events from local .ics or Google Calendar API
- add_event(title, date, time, duration) - Adds event
- get_upcoming_events(days=7) - Next N days
- Checks CALENDAR_READ/CALENDAR_WRITE permissions
- Supports both local .ics file and Google Calendar API

**agents/tools.py:**
- 10 LangChain Tool instances for LangGraph integration:
  - search_tool, browse_tool, file_read_tool, file_write_tool, file_list_tool
  - calendar_tool, calendar_add_tool, terminal_tool
  - research_tool, memory_query_tool
- get_all_tools() - Returns all tools
- register_tools_with_graph() - Registers with LangGraph agent

Phase 2 COMPLETE.

### Phase 3 — Vision + Intelligence
- [ ] Live camera capture (OpenCV)
- [ ] Frame sampling and motion detection
- [ ] Vision engine integration (Gemini Vision / GPT-4V)
- [ ] Screen capture mode
- [ ] Voice pipeline — STT via Whisper local, TTS via Coqui local
- [ ] Wake word detection
- [ ] Real-time voice conversation loop (low latency)
- [ ] Long-term memory compression (improve existing compress_old_sessions)
- [ ] Proactive trigger intelligence — pattern detection from conversation history
- [ ] vision/stream.py — camera capture + frame sampling
- [ ] vision/processor.py — vision engine integration
- [ ] delivery/voice.py — full voice pipeline
- [ ] Update agents/nodes.py search_node and code_node (stubs from Phase 1)
- [ ] Integration test: end-to-end conversation with engine online

---

## Key Decisions Log

| Decision | Choice | Reason |
|---|---|---|
| Backend language | Python | Best AI library ecosystem |
| LLM strategy | Multi-engine routing | No single model is best at everything |
| Auth (OpenAI/Google) | OAuth2 | More robust than raw API keys |
| Auth (Anthropic) | API Key | OAuth not yet supported |
| Agent framework | LangGraph + LangChain | Multi-step task graphs, doesn't collapse |
| Vector DB | Supabase (primary) + Chroma (fallback) | Free tier + local fallback |
| Voice STT | Whisper local | Free, accurate, private |
| Voice TTS | Coqui local | Free, no API cost |
| Browsing | Playwright + browser-use | Open source, no API cost |
| Search | DuckDuckGo + SearXNG | No API key needed |
| Cost strategy | Maximize free tiers | No mandatory paid services |
| Project name | Orion | — |

---

## Code Style Rules

Established during sessions — all AI assistants must follow:

- No emotes or emoji anywhere in code, comments, logs, or output strings
- All output must be plain readable text
- Log format: `[TIMESTAMP] [LEVEL] [MODULE] message` — no decorative characters
- Comments must be clear prose, not clever or casual
- Built for readability and long-term scalability — assume another engineer will read this in 2 years

---

## How to Continue in a New Session

1. Open VSCode with MCP Claude connected to this repo
2. Say: **"Read SKILL.md and docs/session-log.md then continue Orion"**
3. Claude will know exactly where we are and what to do next

Current state: Phase 2 complete. Phase 3 is next — vision layer (camera + screen capture), voice pipeline (Whisper + Coqui), and proactive pattern intelligence.

Phase 3 priority order: voice pipeline first (fastest to test), then vision layer, then proactive pattern intelligence.

---

*Last updated: February 2026 — Phase 2 complete, Phase 3 starting*
