# Orion — Session Log & Conversation Documentation

> This file documents all major decisions, directions, and progress made across all sessions.
> Update this file at the end of every session before pushing.
> If starting a new session with a new AI assistant, read SKILL.md + this file first.

---

## Session 1 — Project Genesis
**Date:** February 2026  
**Tools used:** Claude (claude.ai)

### What was decided

**Vision origin:** Inspired by AI from science fiction — JARVIS (Iron Man), TARS (Interstellar). The goal is not to build another chatbot but a persistent AI companion that behaves like a real intelligent partner.

**Core differentiation identified:**
- Most AI tools are reactive — wait to be asked
- Orion is proactive — reaches out like a friend via WhatsApp-style persistent threads
- Not one message then done — stateful conversation threads with follow-up capability
- Runs as a background daemon, always on, not just when app is open

**Project named:** Orion  
**Repo created:** github.com/maxvyquincy9393/orion

---

## Session 2 — Architecture Decisions

### What was decided

**Backend language:** Python — chosen because all top AI libraries (LangChain, LangGraph, Whisper, vector DB clients) are Python-native. Fastest path to build.

**Multi-engine LLM strategy:** Not locked to one model. Smart routing per task type:
- Claude → complex reasoning, long context
- GPT-4o → code, structured tasks
- Gemini → multimodal, vision, voice
- Ollama → local, free, privacy-sensitive tasks

**Auth approach:**
- OpenAI → OAuth2 Authorization Code Flow
- Google Gemini → OAuth2
- Anthropic → API Key (OAuth not yet supported)
- Ollama → no auth, localhost

**Agent framework:** LangGraph + LangChain  
- LangChain for RAG pipeline and chains  
- LangGraph for multi-step autonomous agent tasks  
- Enables complex task graphs that don't collapse mid-execution

**Vision layer added:** Live camera + screen capture, like Gemini Live  
- OpenCV for capture  
- Frame sampling with motion detection (not every frame)  
- Gemini Vision or GPT-4V for processing  
- 4 modes: passive / active / on-demand / screen

**Permission sandbox added:** Every system action must pass through sandbox.check() first  
- User edits permissions.yaml to control what Orion can and cannot do  
- Granular per action: FILE_READ, FILE_WRITE, FILE_DELETE, TERMINAL_RUN, APP_OPEN, etc.  
- require_confirm: true = Orion asks via Telegram before acting  
- Hot reload — change permissions without restarting

**Autonomous browsing — 100% free:**
- Playwright + browser-use for navigation
- DuckDuckGo (no API key needed)
- SearXNG self-hosted as search backend

**System control capabilities:**
- File read/write/delete
- Terminal commands (with confirm)
- App open/close
- Calendar read/write
- Mouse/keyboard (disabled by default)

**Cost strategy:** Maximize free tiers and open source throughout. No mandatory paid services.
- Whisper → local, free
- Coqui TTS → local, free
- Chroma → local vector DB, free fallback
- Supabase → free tier primary vector DB

---

## Session 3 — AI Coding Stack Setup

### What was decided

**AI coding assistant stack:**
- GitHub Copilot → inline suggestions in VSCode
- OpenCode → Claude-powered, complex reasoning and refactor
- Gemini CLI → backup when Copilot hits limit
- Codex → scaffold and generate large file structures

**Switching strategy:**
- Copilot limit hit → switch to OpenCode (Claude Haiku 4.5)
- OpenCode limit → Gemini CLI
- Codex for scaffold only, not complex logic

**Auto-push workflow:** `scripts/autopush.py` using watchdog  
- Every file save triggers: git add → git commit → git push  
- 3 second debounce to avoid spam commits  
- Commit format: `auto: [timestamp] [filename]`

**MANDATORY RULES established (in SKILL.md):**
1. Always commit — every change, even WIP
2. Always document — every file, function, and decision

---

## Session 4 — Build Progress

### Phase 1 Implementation Log

**Scaffold** — 43 files, 3028 lines generated by Codex  
All interfaces match SKILL.md exactly.

**config.py** ✅
- Loads .env via python-dotenv
- Typed module-level constants grouped into 8 sections
- validate_required_for_engine() per engine on demand
- as_dict() safe debug dump with masked secrets

**permissions/permission_types.py** ✅
- PermissionAction enum with 11 actions
- ACTION_TO_SECTION mapping
- PermissionResult frozen dataclass

**permissions/config_loader.py** ✅
- Thread-safe RLock
- Hot reload via reload()
- Validates all 11 required sections on load

**permissions/sandbox.py** ✅
- check(action, details) → PermissionResult
- Per-action logic: file path filtering, blocked commands, allowed apps, domain blocking
- request_confirm() → sends Telegram message, polls 30s for yes/no
- Logs to logs/permissions.log

**database/models.py** ✅
- 7 ORM models: User, Message, Session, Memory, Thread, CompressedMemory, TriggerLog
- UUID primary keys, timezone-aware DateTime
- MessageRole enum, ThreadState enum
- Composite indexes for performance
- get_engine(), get_session(), create_all_tables(), drop_all_tables()

**database/vector_store.py** ✅
- Dual backend: Supabase pgvector (primary) + Chroma local (fallback)
- Auto-detection based on config
- Embedding via LangChain (OpenAI or Ollama nomic-embed-text)
- embed(), upsert(), search(), delete(), get_store_stats()

**core/memory.py** ✅
- save_message() — dual write PostgreSQL + vector store
- get_history() — PostgreSQL ordered by timestamp
- get_relevant_context() — vector similarity search scoped to user_id
- compress_old_sessions() — LLM summarization, stores CompressedMemory, deletes old messages

**engines/base.py** ✅
- Abstract base class
- format_messages() concrete method — converts to OpenAI message format

**engines/openai_engine.py** ✅ — OAuth2 via OPENAI_ACCESS_TOKEN  
**engines/claude_engine.py** ✅ — API key auth, system prompt handling  
**engines/gemini_engine.py** ✅ — OAuth2 via GOOGLE_ACCESS_TOKEN  
**engines/local_engine.py** ✅ — Ollama REST API, streaming via JSON lines

**core/rag.py** ✅
- ingest() — chunk via RecursiveCharacterTextSplitter (512/50), embed, upsert
- ingest_file() — auto-detect pdf/txt/md/docx, load, chunk, ingest
- query() — embed question, vector search, return top_k
- build_context() — formatted context string with source attribution
- delete_document()

**core/orchestrator.py** ✅
- route(task_type) — priority routing per task
- route_to_agent(task) — keyword-based agent type detection
- get_available_engines() — startup availability check

**core/context.py** done
- build() assembles full context: last 20 messages + RAG context + relevant past context + system prompt
- get_system_prompt() defines Orion personality
- truncate_context() handles context window limits

**agents/state.py** done
- AgentState TypedDict matching SKILL.md exactly

**agents/nodes.py** done
- supervisor_node, memory_node, summarize_node fully implemented
- search_node, code_node as stubs (Phase 2)

**agents/graph.py** done
- OrionAgentGraph with LangGraph StateGraph
- Nodes: start → supervisor → [search, memory, summarize] → end
- run() and stream_run() implemented

**scripts/autopush.py** done
- AutoPusher class with watchdog, 3 second debounce
- Ignores .env, __pycache__, logs/, .git/
- Logs to logs/autopush.log

**main.py** done
- CLI chat loop: input → context.build() → orchestrator.route() → engine.generate() → memory.save_message()
- Handles exit and quit
- Prints engine status on startup

### Phase 1 first run results (python main.py)

| Component | Status | Note |
|---|---|---|
| Database | Failed | PostgreSQL not running on localhost:5432 — expected |
| Claude | Offline | No ANTHROPIC_API_KEY in .env — expected |
| OpenAI | Offline | No OPENAI_ACCESS_TOKEN — expected |
| Gemini | Offline | No GOOGLE_ACCESS_TOKEN — expected |
| Local | Offline | Ollama not running — expected |
| CLI Loop | Working | Prompts for input, exits on quit |
| Permissions | Working | Defaults loaded |

All failures are expected — no credentials set up yet. Architecture is sound.

### Next steps to get first live conversation
1. Add ANTHROPIC_API_KEY to .env for fastest path to working engine
2. OR run `ollama serve` and `ollama pull llama3` for free local inference
3. Run `python main.py` again — first real conversation with Orion

---

## Phase Checklist

### Phase 1 — Foundation
- [x] OAuth2 setup structure (openai_oauth.py, google_oauth.py)
- [x] Project scaffold — 43 files
- [x] Permission engine (sandbox, config_loader, permission_types, permissions.yaml)
- [x] PostgreSQL schema and SQLAlchemy models
- [x] Vector DB setup (Supabase + Chroma fallback)
- [x] RAG pipeline (ingest, query, build_context)
- [x] All 4 LLM engines (GPT, Claude, Gemini, Ollama)
- [x] Orchestrator routing
- [x] Persistent memory (save, retrieve, compress)
- [x] context.py — context window builder
- [x] agents/ — LangGraph scaffold (state, graph, nodes)
- [x] scripts/autopush.py
- [x] main.py — first working CLI chat loop
- [x] docs/ folder complete

Phase 1 COMPLETE.

### Phase 2 — Proactive Engine
- [x] Background daemon process (background/process.py - OrionDaemon class)
- [x] Trigger detection system (background/triggers.py - TriggerEngine, TriggerType enum)
- [x] Thread state manager (background/thread_manager.py - open_thread, update_state, get_pending_threads, should_follow_up)
- [x] Telegram delivery (delivery/messenger.py - send, send_with_confirm, get_latest_reply, set_webhook)
- [x] Autonomous browser agent (browser/agent.py - BrowserAgent with search_and_summarize, research, navigate_and_extract)
- [x] Free search (browser/search.py - DuckDuckGo HTML scrape + SearXNG fallback, no API key needed)
- [x] System control: file, terminal, calendar (system/file_ops.py, system/terminal.py, system/calendar_ops.py)
- [x] Permission confirmation flow (all system/browser actions call sandbox.check() and sandbox.request_confirm())
- [x] LangGraph tools: search, browse, file, calendar (agents/tools.py - 10 tools registered)

Phase 2 COMPLETE.

**Phase 2 Implementation Details:**

**delivery/messenger.py:**
- send() - Telegram Bot API message sending
- send_with_confirm() - Send + poll for yes/no reply
- get_latest_reply() - Poll for new messages
- set_webhook() - Production webhook setup
- Logs to logs/delivery.log

**background/triggers.py:**
- TriggerType enum: TIME_BASED, SCHEDULE, PATTERN, INACTIVITY, KEYWORD
- Trigger dataclass with id, type, condition, message_template, last_fired, enabled
- TriggerEngine class with load_triggers(), evaluate(), get_fired_triggers(), build_message(), mark_fired()
- Default triggers.yaml: morning check-in (8am), inactivity (4 hours), end of day summary (6pm)
- Logs to logs/triggers.log

**background/thread_manager.py:**
- open_thread(user_id, trigger) - Creates Thread record, returns thread_id
- update_state(thread_id, state) - Updates Thread.state (open/waiting/resolved)
- get_pending_threads(user_id) - Returns all non-resolved threads
- should_follow_up(thread_id) - True if waiting > 1 hour
- Logs to logs/threads.log

**background/process.py:**
- OrionDaemon class with start(), stop(), _loop()
- Runs every 60 seconds in separate thread
- Builds context: current time, day, last message time, pending threads
- Calls trigger_engine.get_fired_triggers()
- For each trigger: checks sandbox permission, opens thread, sends message
- Checks pending threads for follow-ups
- Respects quiet_hours (22:00 - 08:00 by default)
- Logs to logs/daemon.log

**browser/search.py:**
- search(query, max_results) - DuckDuckGo HTML scrape, SearXNG fallback
- _duckduckgo_search() - No API key, pure HTML scraping
- _searxng_search() - Self-hosted search backend
- search_images(), search_news() helper functions
- Logs to logs/browser.log

**browser/playwright_client.py:**
- PlaywrightClient class with async context manager support
- navigate(url) - Return page text content
- screenshot(url) - Return PNG bytes
- extract_links(url) - Return all href links
- extract_content(url, selector) - Scoped text extraction
- click(), fill(), wait_for_selector() for interaction
- All actions check BROWSER_NAVIGATE permission
- Logs to logs/browser.log

**browser/agent.py:**
- BrowserAgent class
- search_and_summarize(query) - Search, visit top 3, summarize via LLM
- research(topic, depth=2) - Multi-level research with link following
- navigate_and_extract(url, goal) - Goal-oriented extraction
- All actions check sandbox permission
- Logs to logs/browser.log

**system/file_ops.py:**
- read_file(path) - Returns file contents
- write_file(path, content) - Writes content
- delete_file(path) - Deletes file
- list_dir(path) - Lists directory
- create_dir(path), copy_file(src, dst), file_exists(), get_file_info()
- Every function calls sandbox.check() with FILE_READ/FILE_WRITE/FILE_DELETE

**system/terminal.py:**
- run(command, timeout) - Returns stdout, stderr, exit_code
- run_safe(command) - Always checks blocked_commands list
- run_background(command) - Returns process ID
- BLOCKED_COMMANDS list: rm -rf, sudo, format, etc.
- Calls sandbox.check(TERMINAL_RUN) with request_confirm()

**system/calendar_ops.py:**
- get_events(date) - Returns events from local .ics or Google Calendar API
- add_event(title, date, time, duration) - Adds event
- get_upcoming_events(days=7) - Next N days
- Checks CALENDAR_READ/CALENDAR_WRITE permissions
- Supports both local .ics file and Google Calendar API

**agents/tools.py:**
- 10 LangChain Tool instances for LangGraph integration:
  - search_tool, browse_tool, file_read_tool, file_write_tool, file_list_tool
  - calendar_tool, calendar_add_tool, terminal_tool
  - research_tool, memory_query_tool
- get_all_tools() - Returns all tools
- register_tools_with_graph() - Registers with LangGraph agent

Phase 2 COMPLETE.

### Phase 3 — Vision + Intelligence
- [x] Live camera capture (OpenCV) - vision/stream.py
- [x] Frame sampling and motion detection - vision/stream.py
- [x] Vision engine integration (Gemini Vision / GPT-4V) - vision/processor.py
- [x] Screen capture mode - vision/stream.py
- [x] Voice pipeline — STT via Whisper local, TTS via Coqui local - delivery/voice.py
- [x] Wake word detection - delivery/voice.py detect_wake_word()
- [x] Real-time voice conversation loop (low latency) - delivery/voice.py conversation_loop()
- [x] Voice cloning - delivery/voice.py VoicePipeline with clone_voice_from_file(), train_voice_model()
- [x] Proactive trigger intelligence — pattern detection from conversation history - core/intelligence.py
- [x] vision/stream.py — CameraStream with start(), stop(), get_frame(), capture_screenshot(), sample_frames(), detect_motion(), frame_to_base64(), save_frame()
- [x] vision/processor.py — VisionProcessor with analyze_frame(), analyze_screen(), watch_and_describe(), extract_text_from_frame(), compare_frames()
- [x] delivery/voice.py — Full VoicePipeline class with listen(), speak(), conversation_loop(), detect_wake_word(), plus voice cloning methods
- [x] core/intelligence.py — PatternIntelligence class with analyze_history(), suggest_proactive_actions(), update_trigger_weights(), get_user_summary()
- [x] Update agents/nodes.py search_node and code_node - fully implemented
- [x] Update main.py — argparse CLI with --mode text|voice|vision|all
- [x] requirements.txt — Added TTS, openai-whisper, sounddevice, soundfile, numpy, opencv-python, mss, pytesseract, Pillow

Phase 3 COMPLETE.

**Phase 3 Implementation Details:**

**delivery/voice.py:**
- VoicePipeline class with full voice interaction capabilities
- listen() - Record audio with silence detection, transcribe via Whisper
- speak() - Text-to-speech via Coqui XTTS-v2
- conversation_loop() - Continuous wake-word-activated conversation
- detect_wake_word() - Listen for wake word in 2-second windows
- Voice cloning: record_training_samples(), train_voice_model(), clone_voice_from_file()
- list_voice_profiles(), speak_with_clone(), auto_train_from_conversation()
- Voice profiles stored in models/voices/[voice_name]/ with metadata.json
- Logs to logs/voice.log and logs/voice_training.log

**vision/stream.py:**
- CameraStream class with OpenCV camera capture
- start(), stop() - Start/stop camera stream
- get_frame() - Get current frame as numpy array
- capture_screenshot() - Capture screen region via mss
- sample_frames() - Sample frames at interval with motion detection
- detect_motion() - Compare consecutive frames for changes
- frame_to_base64() - Convert frame to base64 string
- save_frame() - Save frame to file
- Logs to logs/vision.log

**vision/processor.py:**
- VisionProcessor class with multimodal analysis
- analyze_frame() - Analyze frame with Gemini Vision (primary) or GPT-4V (fallback)
- analyze_screen() - Capture and analyze screen region
- watch_and_describe() - Continuous frame analysis with callback
- extract_text_from_frame() - OCR via pytesseract
- compare_frames() - Describe differences between frames
- Auto-detects available vision engine
- Logs to logs/vision.log

**core/intelligence.py:**
- PatternIntelligence class for proactive pattern detection
- analyze_history() - Analyze conversation history for patterns (topics, time, keywords, sequences)
- suggest_proactive_actions() - Generate suggestions based on context and time
- update_trigger_weights() - Reinforcement learning for trigger weights
- get_user_summary() - Build user profile summary
- Patterns stored in data/patterns.json
- Trigger weights in data/trigger_weights.json
- Logs to logs/intelligence.log

**agents/nodes.py updates:**
- search_node() - Fully implemented with DuckDuckGo/SearXNG search
- code_node() - Fully implemented with LLM code generation
- _extract_search_query() - Extract clean search query from task
- _build_code_prompt() - Build code generation prompt with memory context
- _get_code_system_prompt() - System prompt for code tasks

**main.py updates:**
- argparse CLI with --mode text|voice|vision|all
- voice_loop() - Voice-based interaction
- vision_loop() - Camera-based interaction with commands
- all_modes_loop() - Combined text/voice/vision modes
- parse_args() - Command line argument parsing
- Graceful error handling for missing optional hardware (camera, microphone)

Phase 3 COMPLETE.

### Phase 4 - Credentials, Setup, and First Live Conversation
- [x] Interactive setup wizard (scripts/setup.py)
- [x] SQLite support for database models (database/models.py - UUIDColumn helper)
- [x] End-to-end smoke test (scripts/first_run.py)
- [x] Setup guide documentation (docs/setup-guide.md)
- [x] TELEGRAM_CHAT_ID added to config.py and .env.example
- [x] DATA_DIR added to config.py
- [x] Simplified .env.example with new API key format

Phase 4 COMPLETE.

**Phase 4 Implementation Details:**

**scripts/setup.py:**
- Interactive CLI wizard for credential configuration
- Prompts for: ANTHROPIC_API_KEY (required), TELEGRAM_BOT_TOKEN (required), TELEGRAM_CHAT_ID (required)
- Optional: OPENAI_API_KEY, GEMINI_API_KEY
- Database URL configuration (SQLite default, PostgreSQL option)
- Connectivity tests for Claude, OpenAI, Gemini, Telegram
- Preserves existing .env values unless user confirms changes
- Prints configuration summary and next steps

**database/models.py:**
- UUIDColumn() helper function for database-agnostic UUID handling
- Detects SQLite vs PostgreSQL via DATABASE_URL
- SQLite: uses String(36) for UUID columns
- PostgreSQL: uses native UUID(as_uuid=True)
- All 7 models updated to use UUIDColumn()
- Allows quick local testing without PostgreSQL

**scripts/first_run.py:**
- End-to-end smoke test
- Tests: database tables, memory save/retrieve, engine availability, conversation, daemon
- Conversation test sends: "Hello, say 'Orion is online' and nothing else"
- Daemon test: start, run for 5 seconds, verify cycles, stop
- Prints detailed summary of all test results

**docs/setup-guide.md:**
- Prerequisites: Python 3.11+, pip, git
- Quick start guide (SQLite + Claude only)
- Full setup guide (PostgreSQL + all engines)
- Telegram bot creation and chat ID instructions
- Voice setup (Whisper + Coqui)
- Vision setup (OpenCV + mss + Tesseract)
- Permissions configuration guide
- Troubleshooting common errors

**config.py updates:**
- Added TELEGRAM_CHAT_ID configuration variable
- Added DATA_DIR path with auto-creation

**.env.example updates:**
- Simplified to use OPENAI_API_KEY and GEMINI_API_KEY (simpler than OAuth tokens)
- Added TELEGRAM_CHAT_ID
- Default DATABASE_URL changed to sqlite:///orion.db

Phase 4 COMPLETE.

---

## Session 5 - Multi-Provider Auth Upgrade
**Date:** February 2026
**Tools used:** Codex

### What was implemented

- Multi-provider auth system completed with new OAuth modules under `engines/auth/`
- OpenAI/Codex OAuth device-code flow implemented with token persistence in `.orion/auth/openai.json`
- Google Gemini OAuth device-code flow implemented with token persistence in `.orion/auth/gemini.json`
- Central `AuthManager` added for provider token resolution, status, login/logout, and Ollama reachability checks
- `openai_engine.py` updated to use `AuthManager` with OAuth Bearer mode and API-key mode fallback
- `gemini_engine.py` updated to use `AuthManager` with OAuth REST mode and API-key SDK mode fallback
- New engines added:
  - `engines/groq_engine.py`
  - `engines/openrouter_engine.py`
- `core/orchestrator.py` integrated with all new providers and routing priorities:
  - reasoning: anthropic > openai > gemini > openrouter > groq > local
  - code: openai > anthropic > groq > openrouter > local
  - fast: groq > gemini > local > anthropic
  - multimodal: gemini > openai > anthropic
  - local: ollama/local
- `scripts/setup.py` replaced with full multi-provider onboarding wizard:
  - Telegram required setup
  - Multi-provider selection with API-key or OAuth path
  - Connectivity testing and summary
  - Database selection (SQLite or PostgreSQL)
- `requirements.txt` updated with `groq` dependency and OpenRouter note

### Outcome

Multi-provider auth and onboarding flow are now complete and aligned with OpenClaw-style provider selection.
OAuth support for OpenAI and Gemini is active.
Groq and OpenRouter engines are integrated into routing.

---

## Key Decisions Log

| Decision | Choice | Reason |
|---|---|---|
| Backend language | Python | Best AI library ecosystem |
| LLM strategy | Multi-engine routing | No single model is best at everything |
| Auth (OpenAI/Google) | OAuth2 | More robust than raw API keys |
| Auth (Anthropic) | API Key | OAuth not yet supported |
| Agent framework | LangGraph + LangChain | Multi-step task graphs, doesn't collapse |
| Vector DB | Supabase (primary) + Chroma (fallback) | Free tier + local fallback |
| Voice STT | Whisper local | Free, accurate, private |
| Voice TTS | Coqui local | Free, no API cost |
| Browsing | Playwright + browser-use | Open source, no API cost |
| Search | DuckDuckGo + SearXNG | No API key needed |
| Cost strategy | Maximize free tiers | No mandatory paid services |
| Project name | Orion | — |

---

## Code Style Rules

Established during sessions — all AI assistants must follow:

- No emotes or emoji anywhere in code, comments, logs, or output strings
- All output must be plain readable text
- Log format: `[TIMESTAMP] [LEVEL] [MODULE] message` — no decorative characters
- Comments must be clear prose, not clever or casual
- Built for readability and long-term scalability — assume another engineer will read this in 2 years

---

## How to Continue in a New Session

1. Open VSCode with MCP Claude connected to this repo
2. Say: **"Read SKILL.md and docs/session-log.md then continue Orion"**
3. Claude will know exactly where we are and what to do next

Current state: All phases complete. Orion is ready for first live run.

To run Orion:
1. python scripts/setup.py - Configure credentials
2. python scripts/first_run.py - Verify everything works
3. python main.py - Start Orion

---

*Last updated: February 2026 - Session 5 multi-provider auth complete*
