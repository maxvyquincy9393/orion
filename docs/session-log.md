# Orion — Session Log & Conversation Documentation

> This file documents all major decisions, directions, and progress made across all sessions.
> Update this file at the end of every session before pushing.
> If starting a new session with a new AI assistant, read SKILL.md + this file first.

---

## Session 1 — Project Genesis
**Date:** February 2026  
**Tools used:** Claude (claude.ai)

### What was decided

**Vision origin:** Inspired by AI from science fiction — JARVIS (Iron Man), TARS (Interstellar). The goal is not to build another chatbot but a persistent AI companion that behaves like a real intelligent partner.

**Core differentiation identified:**
- Most AI tools are reactive — wait to be asked
- Orion is proactive — reaches out like a friend via WhatsApp-style persistent threads
- Not one message then done — stateful conversation threads with follow-up capability
- Runs as a background daemon, always on, not just when app is open

**Project named:** Orion  
**Repo created:** github.com/maxvyquincy9393/orion

---

## Session 2 — Architecture Decisions

### What was decided

**Backend language:** Python — chosen because all top AI libraries (LangChain, LangGraph, Whisper, vector DB clients) are Python-native. Fastest path to build.

**Multi-engine LLM strategy:** Not locked to one model. Smart routing per task type:
- Claude → complex reasoning, long context
- GPT-4o → code, structured tasks
- Gemini → multimodal, vision, voice
- Ollama → local, free, privacy-sensitive tasks

**Auth approach:**
- OpenAI → OAuth2 Authorization Code Flow
- Google Gemini → OAuth2
- Anthropic → API Key (OAuth not yet supported)
- Ollama → no auth, localhost

**Agent framework:** LangGraph + LangChain  
- LangChain for RAG pipeline and chains  
- LangGraph for multi-step autonomous agent tasks  
- Enables complex task graphs that don't collapse mid-execution

**Vision layer added:** Live camera + screen capture, like Gemini Live  
- OpenCV for capture  
- Frame sampling with motion detection (not every frame)  
- Gemini Vision or GPT-4V for processing  
- 4 modes: passive / active / on-demand / screen

**Permission sandbox added:** Every system action must pass through sandbox.check() first  
- User edits permissions.yaml to control what Orion can and cannot do  
- Granular per action: FILE_READ, FILE_WRITE, FILE_DELETE, TERMINAL_RUN, APP_OPEN, etc.  
- require_confirm: true = Orion asks via Telegram before acting  
- Hot reload — change permissions without restarting

**Autonomous browsing — 100% free:**
- Playwright + browser-use for navigation
- DuckDuckGo (no API key needed)
- SearXNG self-hosted as search backend

**System control capabilities:**
- File read/write/delete
- Terminal commands (with confirm)
- App open/close
- Calendar read/write
- Mouse/keyboard (disabled by default)

**Cost strategy:** Maximize free tiers and open source throughout. No mandatory paid services.
- Whisper → local, free
- Coqui TTS → local, free
- Chroma → local vector DB, free fallback
- Supabase → free tier primary vector DB

---

## Session 3 — AI Coding Stack Setup

### What was decided

**AI coding assistant stack:**
- GitHub Copilot → inline suggestions in VSCode
- OpenCode → Claude-powered, complex reasoning and refactor
- Gemini CLI → backup when Copilot hits limit
- Codex → scaffold and generate large file structures

**Switching strategy:**
- Copilot limit hit → switch to OpenCode (Claude Haiku 4.5)
- OpenCode limit → Gemini CLI
- Codex for scaffold only, not complex logic

**Auto-push workflow:** `scripts/autopush.py` using watchdog  
- Every file save triggers: git add → git commit → git push  
- 3 second debounce to avoid spam commits  
- Commit format: `auto: [timestamp] [filename]`

**MANDATORY RULES established (in SKILL.md):**
1. Always commit — every change, even WIP
2. Always document — every file, function, and decision

---

## Session 4 — Build Progress

### Phase 1 Implementation Log

**Scaffold** — 43 files, 3028 lines generated by Codex  
All interfaces match SKILL.md exactly.

**config.py** ✅
- Loads .env via python-dotenv
- Typed module-level constants grouped into 8 sections
- validate_required_for_engine() per engine on demand
- as_dict() safe debug dump with masked secrets

**permissions/permission_types.py** ✅
- PermissionAction enum with 11 actions
- ACTION_TO_SECTION mapping
- PermissionResult frozen dataclass

**permissions/config_loader.py** ✅
- Thread-safe RLock
- Hot reload via reload()
- Validates all 11 required sections on load

**permissions/sandbox.py** ✅
- check(action, details) → PermissionResult
- Per-action logic: file path filtering, blocked commands, allowed apps, domain blocking
- request_confirm() → sends Telegram message, polls 30s for yes/no
- Logs to logs/permissions.log

**database/models.py** ✅
- 7 ORM models: User, Message, Session, Memory, Thread, CompressedMemory, TriggerLog
- UUID primary keys, timezone-aware DateTime
- MessageRole enum, ThreadState enum
- Composite indexes for performance
- get_engine(), get_session(), create_all_tables(), drop_all_tables()

**database/vector_store.py** ✅
- Dual backend: Supabase pgvector (primary) + Chroma local (fallback)
- Auto-detection based on config
- Embedding via LangChain (OpenAI or Ollama nomic-embed-text)
- embed(), upsert(), search(), delete(), get_store_stats()

**core/memory.py** ✅
- save_message() — dual write PostgreSQL + vector store
- get_history() — PostgreSQL ordered by timestamp
- get_relevant_context() — vector similarity search scoped to user_id
- compress_old_sessions() — LLM summarization, stores CompressedMemory, deletes old messages

**engines/base.py** ✅
- Abstract base class
- format_messages() concrete method — converts to OpenAI message format

**engines/openai_engine.py** ✅ — OAuth2 via OPENAI_ACCESS_TOKEN  
**engines/claude_engine.py** ✅ — API key auth, system prompt handling  
**engines/gemini_engine.py** ✅ — OAuth2 via GOOGLE_ACCESS_TOKEN  
**engines/local_engine.py** ✅ — Ollama REST API, streaming via JSON lines

**core/rag.py** ✅
- ingest() — chunk via RecursiveCharacterTextSplitter (512/50), embed, upsert
- ingest_file() — auto-detect pdf/txt/md/docx, load, chunk, ingest
- query() — embed question, vector search, return top_k
- build_context() — formatted context string with source attribution
- delete_document()

**core/orchestrator.py** ✅
- route(task_type) — priority routing per task
- route_to_agent(task) — keyword-based agent type detection
- get_available_engines() — startup availability check

### Currently implementing (end of Session 4)
- core/context.py
- agents/state.py + graph.py + nodes.py
- scripts/autopush.py
- main.py (CLI chat loop — first end-to-end test)

---

## Phase Checklist

### Phase 1 — Foundation
- [x] OAuth2 setup structure (openai_oauth.py, google_oauth.py)
- [x] Project scaffold — 43 files
- [x] Permission engine (sandbox, config_loader, permission_types, permissions.yaml)
- [x] PostgreSQL schema and SQLAlchemy models
- [x] Vector DB setup (Supabase + Chroma fallback)
- [x] RAG pipeline (ingest, query, build_context)
- [x] All 4 LLM engines (GPT, Claude, Gemini, Ollama)
- [x] Orchestrator routing
- [x] Persistent memory (save, retrieve, compress)
- [ ] context.py — context window builder
- [ ] agents/ — LangGraph scaffold
- [ ] scripts/autopush.py
- [ ] main.py — first working CLI chat loop
- [ ] docs/ folder complete

### Phase 2 — Proactive Engine
- [ ] Background daemon process
- [ ] Trigger detection system
- [ ] Thread state manager
- [ ] Telegram delivery
- [ ] Autonomous browser agent (Playwright + browser-use)
- [ ] Free search (DuckDuckGo + SearXNG)
- [ ] System control: file, terminal, calendar
- [ ] Permission confirmation flow
- [ ] LangGraph tools: search, browse, file, calendar

### Phase 3 — Vision + Intelligence
- [ ] Live camera capture (OpenCV)
- [ ] Frame sampling and motion detection
- [ ] Vision engine integration
- [ ] Screen capture mode
- [ ] Voice pipeline (Whisper local + Coqui TTS)
- [ ] Long-term memory compression
- [ ] Proactive trigger intelligence (research phase)

---

## Key Decisions Log

| Decision | Choice | Reason |
|---|---|---|
| Backend language | Python | Best AI library ecosystem |
| LLM strategy | Multi-engine routing | No single model is best at everything |
| Auth (OpenAI/Google) | OAuth2 | More robust than raw API keys |
| Auth (Anthropic) | API Key | OAuth not yet supported |
| Agent framework | LangGraph + LangChain | Multi-step task graphs, doesn't collapse |
| Vector DB | Supabase (primary) + Chroma (fallback) | Free tier + local fallback |
| Voice STT | Whisper local | Free, accurate, private |
| Voice TTS | Coqui local | Free, no API cost |
| Browsing | Playwright + browser-use | Open source, no API cost |
| Search | DuckDuckGo + SearXNG | No API key needed |
| Cost strategy | Maximize free tiers | No mandatory paid services |
| Project name | Orion | — |

---

## How to Continue in a New Session

1. Open VSCode with MCP Claude connected to this repo
2. Say: **"Read SKILL.md and docs/session-log.md then continue Orion"**
3. Claude will know exactly where we are and what to do next

---

*Last updated: February 2026*
